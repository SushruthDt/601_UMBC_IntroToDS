{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Week10 Part1\n",
    "\n",
    "I provide a .zip containing .txt and .docx files\n",
    "\n",
    "v \n",
    "Produce a single .dat file containing the name of the file in quotes, a colon, then a list of words separated by commas. The list of words per file should be unique. \n",
    "\n",
    "Do not include URLs or phone numbers.\n",
    "```\n",
    "\"File 1.txt\" : word1, word2, word3, word7\n",
    "\"name of file.docx\" : word8, word2, word1, word10\n",
    "\"another file.doc\" : word1, word12, word6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.8.10)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-docx) (4.2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lenovo\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "# # In case of any corpus are missing \n",
    "# download all-nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the Stop words from NLTK package\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# importing all the Punctuation from String Package\n",
    "punctuation=list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "class FileProcessor:\n",
    "    \n",
    "    def __init__(self, extract_path ='text_files', file_types=['.txt', '.docx']):\n",
    "\n",
    "        self.file_url = \"https://github.com/msaricaumbc/DS601_Spring21/raw/main/Week10/homework/text_files.zip\"\n",
    "        self.extract_path = extract_path\n",
    "        self.zip_filename = 'textfiles.zip'\n",
    "        self.file_types = file_types\n",
    "        \n",
    "    def download_file(self):\n",
    "        \"\"\"\n",
    "        download a file based on a URL, save to filename\n",
    "        \"\"\"\n",
    "        r = requests.get(self.file_url , allow_redirects=True)\n",
    "        open(self.zip_filename, 'wb').write(r.content)\n",
    "\n",
    "    def extract_zip(self):\n",
    "        with zipfile.ZipFile(self.zip_filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall()    \n",
    "    \n",
    "    def find_all_files(self):\n",
    "        paths = []\n",
    "        for root, dirs, files in os.walk(self.extract_path):\n",
    "            for file in files:\n",
    "                if any([file.endswith(ft) for ft in self.file_types]):\n",
    "                    path = (os.path.join(root, file), file)\n",
    "                    # print(path)\n",
    "                    paths.append(path)\n",
    "        # print(paths)\n",
    "        return paths\n",
    "\n",
    "    def save_obj(self, obj, file_name):\n",
    "        with open(file_name, \"w\") as f:\n",
    "            for key in obj.keys():\n",
    "                line = key + ':\\t' + ','.join(obj[key]) + '\\n'\n",
    "                f.write(line)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Entry point for the class\n",
    "        \"\"\"\n",
    "        self.download_file()\n",
    "        \n",
    "        self.extract_zip()\n",
    "        \n",
    "        files = self.find_all_files()\n",
    "        \n",
    "        obj = {}\n",
    "        \n",
    "        for path, file in files:\n",
    "            print ('Processing: ', file)\n",
    "            obj[file] = self.process_file(path)\n",
    "    \n",
    "        print('Saving File')\n",
    "        self.save_obj(obj, 'output.dat')\n",
    "        \n",
    "        print('Process Finished')\n",
    "        \n",
    "    def process_file(self, file_name):\n",
    "        \"\"\"\n",
    "        abstract method\n",
    "        \n",
    "        Parameters: file_name\n",
    "        Returns: List of unique words ['test', 'hair']\n",
    "        \"\"\"\n",
    "        if file_name.endswith('.txt'):\n",
    "            with open(file_name, 'r', encoding = 'utf8') as file :\n",
    "                result = file.read()\n",
    "                #result = result.split()\n",
    "    \n",
    "        elif file_name.endswith('.docx'):\n",
    "            import docx\n",
    "            file = docx.Document(file_name)            \n",
    "            result = [i.text for i in file.paragraphs]\n",
    "            print(type(result))\n",
    "            result = \" \".join(result)            \n",
    "            # self.clean_file(words) \n",
    "            \n",
    "             # split into words\n",
    "            \n",
    "        from nltk.tokenize import word_tokenize\n",
    "        tokens = word_tokenize(result)\n",
    "        #print(type(tokens))\n",
    "        \n",
    "                # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        regxUrl=\"https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|http?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}\"\n",
    "        words = [re.sub(regxUrl, '', s, flags=re.MULTILINE) for s in tokens]\n",
    "\n",
    "                # Removing numbers from the word\n",
    "        tokens = [re.sub('[0-9]', '', i) for i in words]\n",
    "\n",
    "\n",
    "                # remove punctuation from each word\n",
    "        import string\n",
    "        tokens_punc = str.maketrans('', '', string.punctuation)\n",
    "        tokens_strip = [w.translate(tokens_punc) for w in tokens]\n",
    "        \n",
    "                # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in tokens_strip if word.isalpha()]\n",
    "        #print(words)\n",
    "        \n",
    "              # filter out stop words\n",
    "        from nltk.corpus import stopwords\n",
    "        stopword = stopwords.words('english')\n",
    "        words = [w for w in words if w not in stopword]\n",
    "        words=set(words)\n",
    "        return words\n",
    "        print(words)\n",
    "               \n",
    "                    #raise NotImplemented('process_file method not implemented'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Assignment1 = FileProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  52256-0.txt\n",
      "Processing:  53031-0.txt\n",
      "Processing:  58108-0.txt\n",
      "Processing:  blind_text.txt\n",
      "Processing:  dr_yawn.txt\n",
      "Processing:  how_rubber_goods_are_made.txt\n",
      "Processing:  most_boring_ever.txt\n",
      "Processing:  most_boring_part2.txt\n",
      "Processing:  pg12814.txt\n",
      "Processing:  pg14895.txt\n",
      "Processing:  pg43994.txt\n",
      "Processing:  random_text.txt\n",
      "Processing:  smiley_the_bunny.txt\n",
      "Processing:  week_10_document1.docx\n",
      "<class 'list'>\n",
      "Processing:  week_10_document2.docx\n",
      "<class 'list'>\n",
      "Saving File\n",
      "Process Finished\n"
     ]
    }
   ],
   "source": [
    "Assignment1.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
